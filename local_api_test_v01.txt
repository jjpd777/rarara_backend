
= * 80
 ğŸ”¥ğŸ”¥ğŸ”¥ PSYCHO MODE LLM API TESTING SUITE ğŸ”¥ğŸ”¥ğŸ”¥
= * 80
âš ï¸  PSYCHO MODE ENABLED - SHOWING EVERY REQUEST AND RESPONSE!
â„¹ï¸  Base URL: http://localhost:4000

= * 80
 ğŸ“‹ TESTING MODELS LIST ENDPOINT
= * 80

- * 60
 List Available Models
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/models
ğŸ“¤ REQUEST: METHOD: GET
ğŸ“¤ REQUEST: BODY: (empty)

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=648
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "models": [
      {
        "model": "gpt-4.1",
        "display_name": "OpenAI - GPT-4.1",
        "house": "OpenAI",
        "model_type": "text_gen"
      },
      {
        "model": "claude-sonnet-4-20250514",
        "display_name": "Anthropic - Claude Sonnet 4",
        "house": "Anthropic",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Flash",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5",
        "display_name": "Google - Gemini 2.5",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Pro",
        "house": "Google",
        "model_type": "text_gen"
      }
    ]
  },
  "metadata": {
    "timestamp": "2025-07-23T03:59:38.096Z",
    "totalCount": 5
  },
  "success": true
}

âœ… âœ¨ List Available Models - PASSED âœ¨
âœ… Found 5 available models:
  ğŸ¤– gpt-4.1 (OpenAI) - OpenAI - GPT-4.1
  ğŸ¤– claude-sonnet-4-20250514 (Anthropic) - Anthropic - Claude Sonnet 4
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Flash
  ğŸ¤– gemini-2.5 (Google) - Google - Gemini 2.5
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Pro

= * 80
 ğŸ¤– TESTING OPENAI PROVIDER
= * 80

- * 60
 OpenAI - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=399
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_08def5423082435e",
    "content": "Hello, world! ğŸ‘‹ How can I help you today?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:59:41.243Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 11,
      "output": 13,
      "total": 24,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1060
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Hello, world! ğŸ‘‹ How can I help you today?'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 42 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=11, Output=13, Total=24, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1060ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop

Name                           Value
----                           -----
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Prompt Polishing (YOUR USE CASE)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Improve this character creation prompt. Output only the polished prompt: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=652
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_dad9a97bdeb6b97d",
    "content": "Create a detailed character profile for a medieval monk. Consider their background, daily life, personal beliefs, notable skills, appearance, and any inner conflicts or aspirations. Include details about the monastery they belong to, their relationships with fellow monks and the outside world"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:59:44.486Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length"
    },
    "tokens": {
      "input": 21,
      "output": 50,
      "total": 71,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1130
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Prompt Polishing (YOUR USE CASE) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Create a detailed character profile for a medieval monk. Consider their background, daily life, pers...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 293 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=21, Output=50, Total=71, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1130ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Low Token Limit Stress Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Brief response please",
  "options": {
    "temperature": 0.5,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=496
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_d728974178f84d64",
    "content": "Of course! How can I help you?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:59:47.7Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.5,
      "finishReason": "stop",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 50,
      "userRequested": 20
    },
    "tokens": {
      "input": 10,
      "output": 9,
      "total": 19,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1087
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Low Token Limit Stress Test - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Of course! How can I help you?'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 30 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=10, Output=9, Total=19, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1087ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.5, FinishReason=stop
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 50 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - No Options (Default Behavior)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Test without options",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=764
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_0b1128b2061b540e",
    "content": "Of course! Here is a sample test without options:\n\n---\n\n**1. What is the capital city of France?**\n\n---\n\n**2. Who wrote the play \"Romeo and Juliet\"?**\n\n---\n\n**3. What is the chemical symbol for water?**\n\n---\n\n**4. Name the largest planet in our Solar System.**\n\n---\n\n**5. In what year did the"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:59:51.111Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 75
    },
    "tokens": {
      "input": 10,
      "output": 75,
      "total": 85,
      "maxRequested": 75
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1305
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - No Options (Default Behavior) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Of course! Here is a sample test without options:

---

**1. What is the capital city of France?**

...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 292 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=10, Output=75, Total=85, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 1305ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ§  TESTING ANTHROPIC PROVIDER
= * 80

- * 60
 Anthropic - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=616
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_92a1b09c0259f9ac",
    "content": "Hello! It's nice to meet you. How are you doing today? Is there anything I can help you with or would you like to chat about?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:59:54.961Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "end_turn",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 75,
      "userRequested": 50
    },
    "tokens": {
      "input": 11,
      "output": 34,
      "total": 45,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "timing": {
      "responseMs": 1770
    },
    "model": "claude-sonnet-4-20250514"
  },
  "success": true
}

âœ… âœ¨ Anthropic - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Hello! It's nice to meet you. How are you doing today? Is there anything I can help you with or woul...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 125 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=11, Output=34, Total=45, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 1770ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=end_turn
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 50 â†’ 75 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Anthropic - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Polish this: Medieval monk character",
  "options": {
    "temperature": 0.6,
    "max_tokens": 75
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=732
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_893b5bce13f08ee6",
    "content": "Here are several polished versions of a medieval monk character, depending on your needs:\n\n## Brother Aldric the Illuminator\n\n**Background:** A scholarly Benedictine monk in his 40s who serves as the abbey's head scribe and illuminator. Once the youngest son of a minor noble family, he joined the monastery after a profound spiritual experience"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:00.35Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.6,
      "finishReason": "max_tokens"
    },
    "tokens": {
      "input": 13,
      "output": 75,
      "total": 88,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "timing": {
      "responseMs": 3303
    },
    "model": "claude-sonnet-4-20250514"
  },
  "success": true
}

âœ… âœ¨ Anthropic - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Here are several polished versions of a medieval monk character, depending on your needs:

## Brothe...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 345 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=13, Output=75, Total=88, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 3303ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.6, FinishReason=max_tokens
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’ TESTING GEMINI PROVIDER
= * 80

- * 60
 Gemini - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 100
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=465
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_2a9f4f9d582e94a4",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:05.727Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 4,
      "output": 0,
      "total": 103,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 3286
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=4, Output=0, Total=103, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 3286ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Polish this prompt: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 150
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=465
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_c6bbcc88dde4715c",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:11.02Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 6,
      "output": 0,
      "total": 155,
      "maxRequested": 150
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 3171
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=6, Output=0, Total=155, MaxRequested=150
â„¹ï¸  â±ï¸  TIMING: 3171ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Very Low Tokens (Edge Case)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Brief response",
  "options": {
    "temperature": 0.7,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=576
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_eda70db9f5f5fc10",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:15.867Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 100,
      "userRequested": 20
    },
    "tokens": {
      "input": 2,
      "output": 0,
      "total": 101,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 2757
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Very Low Tokens (Edge Case) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=2, Output=0, Total=101, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 2757ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 100 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’¥ TESTING ERROR HANDLING (EXPECTING 400s)
= * 80

- * 60
 Invalid Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "invalid-model-name",
  "prompt": "Test",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Invalid Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Prompt Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Prompt Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "prompt": "test"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

= * 80
 ğŸ”§ TESTING SMART TOKEN ALLOCATION
= * 80
â„¹ï¸  Testing provider-specific token minimums...

- * 60
 Gemini - Token Adjustment Verification
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Short prompt",
  "options": {
    "max_tokens": 30
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=576
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_305fd65d023189b0",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:29.446Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 100,
      "userRequested": 30
    },
    "tokens": {
      "input": 2,
      "output": 0,
      "total": 101,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 2128
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Token Adjustment Verification - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=2, Output=0, Total=101, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 2128ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 30 â†’ 100 (Provider minimum tokens for reliable generation)
âœ… ğŸ¯ SMART TOKEN ALLOCATION WORKING PERFECTLY!
â„¹ï¸  User requested: 30 tokens
â„¹ï¸  Provider used: 100 tokens
â„¹ï¸  Reason: Provider minimum tokens for reliable generation

= * 80
 ğŸ“ VALIDATING RESPONSE STRUCTURE
= * 80

- * 60
 Response Structure Validation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Test structure",
  "options": {
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=587
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_fead3aa72094549e",
    "content": "Certainly! \"Test structure\" can refer to different contexts. Here are a few common interpretations:\n\n---\n\n### 1. **General Test Structure (Exams/Assessments):**\nA typical test or exam is structured as follows:\n\n- **Title/"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:00:32.528Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length"
    },
    "tokens": {
      "input": 9,
      "output": 50,
      "total": 59,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 960
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ Response Structure Validation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Certainly! "Test structure" can refer to different contexts. Here are a few common interpretations:
...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 221 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=9, Output=50, Total=59, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 960ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
â„¹ï¸  Checking top-level fields: success, data, metadata
âœ… âœ… Found field: success
âœ… âœ… Found field: data
âœ… âœ… Found field: metadata
â„¹ï¸  Checking data fields: content, generationId
âœ… âœ… Found data field: content
âœ… âœ… Found data field: generationId
â„¹ï¸  Checking metadata fields: model, provider, tokens, config, timing
âœ… âœ… Found metadata field: model
âœ… âœ… Found metadata field: provider
âœ… âœ… Found metadata field: tokens
âœ… âœ… Found metadata field: config
âœ… âœ… Found metadata field: timing
âœ… ğŸ‰ RESPONSE STRUCTURE VALIDATION PASSED!

= * 80
 ğŸ”¥ PSYCHO MODE TEST RESULTS SUMMARY ğŸ”¥
= * 80

ğŸ“Š TOTAL TESTS RUN: 15
âœ… ğŸ‰ PASSED: 12
âŒ ğŸ’¥ FAILED: 3
ğŸ“ˆ SUCCESS RATE: 80%

âš ï¸  âš ï¸  Some tests failed. Review the detailed output above.
â„¹ï¸  ğŸ”§ Common issues:
  - Ensure Phoenix server is running (mix phx.server)
  - Check API keys are configured in .env file
  - Verify database is running and migrated

â„¹ï¸  ğŸ”¥ PSYCHO MODE OPTIONS:
â„¹ï¸  Normal mode (less verbose): .\unifiedAiApi.ps1 -Quiet
â„¹ï¸  Skip models test: .\unifiedAiApi.ps1 -SkipModelsTest
â„¹ï¸  Test production: .\unifiedAiApi.ps1 -BaseUrl 'https://ra-backend.fly.dev'

