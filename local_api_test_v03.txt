
= * 80
 ğŸ”¥ğŸ”¥ğŸ”¥ PSYCHO MODE LLM API TESTING SUITE ğŸ”¥ğŸ”¥ğŸ”¥
= * 80
âš ï¸  PSYCHO MODE ENABLED - SHOWING EVERY REQUEST AND RESPONSE!
â„¹ï¸  Base URL: http://localhost:4000

= * 80
 ğŸ“‹ TESTING MODELS LIST ENDPOINT
= * 80

- * 60
 List Available Models
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/models
ğŸ“¤ REQUEST: METHOD: GET
ğŸ“¤ REQUEST: BODY: (empty)

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=648
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "models": [
      {
        "model": "gpt-4.1",
        "display_name": "OpenAI - GPT-4.1",
        "house": "OpenAI",
        "model_type": "text_gen"
      },
      {
        "model": "claude-sonnet-4-20250514",
        "display_name": "Anthropic - Claude Sonnet 4",
        "house": "Anthropic",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Flash",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5",
        "display_name": "Google - Gemini 2.5",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Pro",
        "house": "Google",
        "model_type": "text_gen"
      }
    ]
  },
  "metadata": {
    "timestamp": "2025-07-23T04:36:47.92Z",
    "totalCount": 5
  },
  "success": true
}

âœ… âœ¨ List Available Models - PASSED âœ¨
âœ… Found 5 available models:
  ğŸ¤– gpt-4.1 (OpenAI) - OpenAI - GPT-4.1
  ğŸ¤– claude-sonnet-4-20250514 (Anthropic) - Anthropic - Claude Sonnet 4
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Flash
  ğŸ¤– gemini-2.5 (Google) - Google - Gemini 2.5
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Pro

= * 80
 ğŸ¤– TESTING OPENAI PROVIDER
= * 80

- * 60
 OpenAI - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=392
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "Medieval European monk, 12th century",
    "generationId": "req_fc063a588696441a"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:51.871Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 9,
      "total": 43,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 1834
    }
  },
  "success": true
}

âœ… âœ¨ OpenAI - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Medieval European monk, 12th century'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 36 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=34, Output=9, Total=43, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1834ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop

Name                           Value
----                           -----
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Prompt Polishing (YOUR USE CASE)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=400
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk in 12th-century Western Europe",
    "generationId": "req_14da4a7ab8cb7ea7"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:54.781Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 10,
      "total": 44,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 822
    }
  },
  "success": true
}

âœ… âœ¨ OpenAI - Prompt Polishing (YOUR USE CASE) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Medieval monk in 12th-century Western Europe'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 44 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=34, Output=10, Total=44, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 822ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Low Token Limit Stress Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.5,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=514
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk, specify time & region in history.",
    "generationId": "req_4fb43838237a8a30"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:57.502Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.5,
      "finishReason": "stop",
      "userRequested": 20,
      "providerAdjusted": 50,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 34,
      "output": 11,
      "total": 45,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 517
    }
  },
  "success": true
}

âœ… âœ¨ OpenAI - Low Token Limit Stress Test - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Medieval monk, specify time & region in history.'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 48 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=34, Output=11, Total=45, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 517ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.5, FinishReason=stop
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 50 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - No Options (Default Behavior)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=484
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "Medieval European monk, 12th century",
    "generationId": "req_1a4d9c6b597fb55f"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:00.158Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop",
      "providerAdjusted": 100,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 34,
      "output": 9,
      "total": 43,
      "maxRequested": 100
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 550
    }
  },
  "success": true
}

âœ… âœ¨ OpenAI - No Options (Default Behavior) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Medieval European monk, 12th century'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 36 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=34, Output=9, Total=43, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 550ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ§  TESTING ANTHROPIC PROVIDER
= * 80

- * 60
 Anthropic - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=529
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "12th century European monastery scribe",
    "generationId": "req_e1153f11759bca61"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:03.996Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "end_turn",
      "userRequested": 50,
      "providerAdjusted": 75,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 38,
      "output": 11,
      "total": 49,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "model": "claude-sonnet-4-20250514",
    "timing": {
      "responseMs": 1774
    }
  },
  "success": true
}

âœ… âœ¨ Anthropic - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '12th century European monastery scribe'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 38 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=38, Output=11, Total=49, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 1774ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=end_turn
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 50 â†’ 75 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Anthropic - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.6,
    "max_tokens": 75
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=419
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "12th century European monastery scribe",
    "generationId": "req_36b9dd4c8192822a"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:07.71Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.6,
      "finishReason": "end_turn"
    },
    "tokens": {
      "input": 38,
      "output": 11,
      "total": 49,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "model": "claude-sonnet-4-20250514",
    "timing": {
      "responseMs": 1619
    }
  },
  "success": true
}

âœ… âœ¨ Anthropic - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '12th century European monastery scribe'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 38 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=38, Output=11, Total=49, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 1619ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.6, FinishReason=end_turn
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’ TESTING GEMINI PROVIDER
= * 80

- * 60
 Gemini - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 400
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=466
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_7fa4afa589f8f024"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:14.871Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 427,
      "maxRequested": 400
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 5092
    }
  },
  "success": true
}

âœ… âœ¨ Gemini - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=28, Output=0, Total=427, MaxRequested=400
â„¹ï¸  â±ï¸  TIMING: 5092ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 400
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=466
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_5308953a14849852"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:22.249Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 427,
      "maxRequested": 400
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 5282
    }
  },
  "success": true
}

âœ… âœ¨ Gemini - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=28, Output=0, Total=427, MaxRequested=400
â„¹ï¸  â±ï¸  TIMING: 5282ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Very Low Tokens (Edge Case)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=577
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_4b8f720122e78f58"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:28.35Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "userRequested": 20,
      "providerAdjusted": 300,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 327,
      "maxRequested": 300
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 4019
    }
  },
  "success": true
}

âœ… âœ¨ Gemini - Very Low Tokens (Edge Case) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=28, Output=0, Total=327, MaxRequested=300
â„¹ï¸  â±ï¸  TIMING: 4019ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 300 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’¥ TESTING ERROR HANDLING (EXPECTING 400s)
= * 80

- * 60
 Invalid Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "invalid-model-name",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Invalid Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Prompt Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Prompt Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

= * 80
 ğŸ”§ TESTING SMART TOKEN ALLOCATION
= * 80
â„¹ï¸  Testing provider-specific token minimums...

- * 60
 Gemini - Token Adjustment Verification
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "max_tokens": 30
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=577
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_8d91dc7df5a1461b"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:43.807Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "userRequested": 30,
      "providerAdjusted": 300,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 327,
      "maxRequested": 300
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 4057
    }
  },
  "success": true
}

âœ… âœ¨ Gemini - Token Adjustment Verification - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=28, Output=0, Total=327, MaxRequested=300
â„¹ï¸  â±ï¸  TIMING: 4057ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 30 â†’ 300 (Provider minimum tokens for reliable generation)
âœ… ğŸ¯ SMART TOKEN ALLOCATION WORKING PERFECTLY!
â„¹ï¸  User requested: 30 tokens
â„¹ï¸  Provider used: 300 tokens
â„¹ï¸  Reason: Provider minimum tokens for reliable generation

= * 80
 ğŸ“ VALIDATING RESPONSE STRUCTURE
= * 80

- * 60
 Response Structure Validation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=403
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk, specify time & place in history.",
    "generationId": "req_c143c9abcd83dd50"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:46.67Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 11,
      "total": 45,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 766
    }
  },
  "success": true
}

âœ… âœ¨ Response Structure Validation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Medieval monk, specify time & place in history.'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 47 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=34, Output=11, Total=45, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 766ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop
â„¹ï¸  Checking top-level fields: success, data, metadata
âœ… âœ… Found field: success
âœ… âœ… Found field: data
âœ… âœ… Found field: metadata
â„¹ï¸  Checking data fields: content, generationId
âœ… âœ… Found data field: content
âœ… âœ… Found data field: generationId
â„¹ï¸  Checking metadata fields: model, provider, tokens, config, timing
âœ… âœ… Found metadata field: model
âœ… âœ… Found metadata field: provider
âœ… âœ… Found metadata field: tokens
âœ… âœ… Found metadata field: config
âœ… âœ… Found metadata field: timing
âœ… ğŸ‰ RESPONSE STRUCTURE VALIDATION PASSED!

= * 80
 ğŸ”¥ PSYCHO MODE TEST RESULTS SUMMARY ğŸ”¥
= * 80

ğŸ“Š TOTAL TESTS RUN: 15
âœ… ğŸ‰ PASSED: 12
âŒ ğŸ’¥ FAILED: 3
ğŸ“ˆ SUCCESS RATE: 80%

âš ï¸  âš ï¸  Some tests failed. Review the detailed output above.
â„¹ï¸  ğŸ”§ Common issues:
  - Ensure Phoenix server is running (mix phx.server)
  - Check API keys are configured in .env file
  - Verify database is running and migrated

â„¹ï¸  ğŸ”¥ PSYCHO MODE OPTIONS:
â„¹ï¸  Normal mode (less verbose): .\unifiedAiApi.ps1 -Quiet
â„¹ï¸  Skip models test: .\unifiedAiApi.ps1 -SkipModelsTest
â„¹ï¸  Test production: .\unifiedAiApi.ps1 -BaseUrl 'https://ra-backend.fly.dev'

