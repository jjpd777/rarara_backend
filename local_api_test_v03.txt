
= * 80
 🔥🔥🔥 PSYCHO MODE LLM API TESTING SUITE 🔥🔥🔥
= * 80
⚠️  PSYCHO MODE ENABLED - SHOWING EVERY REQUEST AND RESPONSE!
ℹ️  Base URL: http://localhost:4000

= * 80
 📋 TESTING MODELS LIST ENDPOINT
= * 80

- * 60
 List Available Models
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/models
📤 REQUEST: METHOD: GET
📤 REQUEST: BODY: (empty)

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=648
📥 RESPONSE: BODY:
{
  "data": {
    "models": [
      {
        "model": "gpt-4.1",
        "display_name": "OpenAI - GPT-4.1",
        "house": "OpenAI",
        "model_type": "text_gen"
      },
      {
        "model": "claude-sonnet-4-20250514",
        "display_name": "Anthropic - Claude Sonnet 4",
        "house": "Anthropic",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Flash",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5",
        "display_name": "Google - Gemini 2.5",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Pro",
        "house": "Google",
        "model_type": "text_gen"
      }
    ]
  },
  "metadata": {
    "timestamp": "2025-07-23T04:36:47.92Z",
    "totalCount": 5
  },
  "success": true
}

✅ ✨ List Available Models - PASSED ✨
✅ Found 5 available models:
  🤖 gpt-4.1 (OpenAI) - OpenAI - GPT-4.1
  🤖 claude-sonnet-4-20250514 (Anthropic) - Anthropic - Claude Sonnet 4
  🤖 gemini-2.5-pro (Google) - Google - Gemini 2.5 Flash
  🤖 gemini-2.5 (Google) - Google - Gemini 2.5
  🤖 gemini-2.5-pro (Google) - Google - Gemini 2.5 Pro

= * 80
 🤖 TESTING OPENAI PROVIDER
= * 80

- * 60
 OpenAI - Basic Generation
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=392
📥 RESPONSE: BODY:
{
  "data": {
    "content": "Medieval European monk, 12th century",
    "generationId": "req_fc063a588696441a"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:51.871Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 9,
      "total": 43,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 1834
    }
  },
  "success": true
}

✅ ✨ OpenAI - Basic Generation - PASSED ✨
ℹ️  📝 CONTENT: 'Medieval European monk, 12th century'
ℹ️  📏 CONTENT LENGTH: 36 characters
ℹ️  🎯 TOKENS: Input=34, Output=9, Total=43, MaxRequested=50
ℹ️  ⏱️  TIMING: 1834ms
ℹ️  🤖 PROVIDER: openai / MODEL: gpt-4.1
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=stop

Name                           Value
----                           -----
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Prompt Polishing (YOUR USE CASE)
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=400
📥 RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk in 12th-century Western Europe",
    "generationId": "req_14da4a7ab8cb7ea7"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:54.781Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 10,
      "total": 44,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 822
    }
  },
  "success": true
}

✅ ✨ OpenAI - Prompt Polishing (YOUR USE CASE) - PASSED ✨
ℹ️  📝 CONTENT: 'Medieval monk in 12th-century Western Europe'
ℹ️  📏 CONTENT LENGTH: 44 characters
ℹ️  🎯 TOKENS: Input=34, Output=10, Total=44, MaxRequested=50
ℹ️  ⏱️  TIMING: 822ms
ℹ️  🤖 PROVIDER: openai / MODEL: gpt-4.1
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=stop
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Low Token Limit Stress Test
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.5,
    "max_tokens": 20
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=514
📥 RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk, specify time & region in history.",
    "generationId": "req_4fb43838237a8a30"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:36:57.502Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.5,
      "finishReason": "stop",
      "userRequested": 20,
      "providerAdjusted": 50,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 34,
      "output": 11,
      "total": 45,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 517
    }
  },
  "success": true
}

✅ ✨ OpenAI - Low Token Limit Stress Test - PASSED ✨
ℹ️  📝 CONTENT: 'Medieval monk, specify time & region in history.'
ℹ️  📏 CONTENT LENGTH: 48 characters
ℹ️  🎯 TOKENS: Input=34, Output=11, Total=45, MaxRequested=50
ℹ️  ⏱️  TIMING: 517ms
ℹ️  🤖 PROVIDER: openai / MODEL: gpt-4.1
ℹ️  🔧 CONFIG: Temperature=0.5, FinishReason=stop
⚠️  ⚙️  TOKEN ADJUSTMENT: 20 → 50 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - No Options (Default Behavior)
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {}
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=484
📥 RESPONSE: BODY:
{
  "data": {
    "content": "Medieval European monk, 12th century",
    "generationId": "req_1a4d9c6b597fb55f"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:00.158Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop",
      "providerAdjusted": 100,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 34,
      "output": 9,
      "total": 43,
      "maxRequested": 100
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 550
    }
  },
  "success": true
}

✅ ✨ OpenAI - No Options (Default Behavior) - PASSED ✨
ℹ️  📝 CONTENT: 'Medieval European monk, 12th century'
ℹ️  📏 CONTENT LENGTH: 36 characters
ℹ️  🎯 TOKENS: Input=34, Output=9, Total=43, MaxRequested=100
ℹ️  ⏱️  TIMING: 550ms
ℹ️  🤖 PROVIDER: openai / MODEL: gpt-4.1
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=stop
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 🧠 TESTING ANTHROPIC PROVIDER
= * 80

- * 60
 Anthropic - Basic Generation
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=529
📥 RESPONSE: BODY:
{
  "data": {
    "content": "12th century European monastery scribe",
    "generationId": "req_e1153f11759bca61"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:03.996Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "end_turn",
      "userRequested": 50,
      "providerAdjusted": 75,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 38,
      "output": 11,
      "total": 49,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "model": "claude-sonnet-4-20250514",
    "timing": {
      "responseMs": 1774
    }
  },
  "success": true
}

✅ ✨ Anthropic - Basic Generation - PASSED ✨
ℹ️  📝 CONTENT: '12th century European monastery scribe'
ℹ️  📏 CONTENT LENGTH: 38 characters
ℹ️  🎯 TOKENS: Input=38, Output=11, Total=49, MaxRequested=75
ℹ️  ⏱️  TIMING: 1774ms
ℹ️  🤖 PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=end_turn
⚠️  ⚙️  TOKEN ADJUSTMENT: 50 → 75 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Anthropic - Prompt Polishing
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.6,
    "max_tokens": 75
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=419
📥 RESPONSE: BODY:
{
  "data": {
    "content": "12th century European monastery scribe",
    "generationId": "req_36b9dd4c8192822a"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:07.71Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.6,
      "finishReason": "end_turn"
    },
    "tokens": {
      "input": 38,
      "output": 11,
      "total": 49,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "model": "claude-sonnet-4-20250514",
    "timing": {
      "responseMs": 1619
    }
  },
  "success": true
}

✅ ✨ Anthropic - Prompt Polishing - PASSED ✨
ℹ️  📝 CONTENT: '12th century European monastery scribe'
ℹ️  📏 CONTENT LENGTH: 38 characters
ℹ️  🎯 TOKENS: Input=38, Output=11, Total=49, MaxRequested=75
ℹ️  ⏱️  TIMING: 1619ms
ℹ️  🤖 PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
ℹ️  🔧 CONFIG: Temperature=0.6, FinishReason=end_turn
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 💎 TESTING GEMINI PROVIDER
= * 80

- * 60
 Gemini - Basic Generation
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 400
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=466
📥 RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_7fa4afa589f8f024"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:14.871Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 427,
      "maxRequested": 400
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 5092
    }
  },
  "success": true
}

✅ ✨ Gemini - Basic Generation - PASSED ✨
ℹ️  📝 CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
ℹ️  📏 CONTENT LENGTH: 95 characters
ℹ️  🎯 TOKENS: Input=28, Output=0, Total=427, MaxRequested=400
ℹ️  ⏱️  TIMING: 5092ms
ℹ️  🤖 PROVIDER: gemini / MODEL: gemini-2.5-pro
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Prompt Polishing
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 400
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=466
📥 RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_5308953a14849852"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:22.249Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 427,
      "maxRequested": 400
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 5282
    }
  },
  "success": true
}

✅ ✨ Gemini - Prompt Polishing - PASSED ✨
ℹ️  📝 CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
ℹ️  📏 CONTENT LENGTH: 95 characters
ℹ️  🎯 TOKENS: Input=28, Output=0, Total=427, MaxRequested=400
ℹ️  ⏱️  TIMING: 5282ms
ℹ️  🤖 PROVIDER: gemini / MODEL: gemini-2.5-pro
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Very Low Tokens (Edge Case)
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 20
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=577
📥 RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_4b8f720122e78f58"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:28.35Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "userRequested": 20,
      "providerAdjusted": 300,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 327,
      "maxRequested": 300
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 4019
    }
  },
  "success": true
}

✅ ✨ Gemini - Very Low Tokens (Edge Case) - PASSED ✨
ℹ️  📝 CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
ℹ️  📏 CONTENT LENGTH: 95 characters
ℹ️  🎯 TOKENS: Input=28, Output=0, Total=327, MaxRequested=300
ℹ️  ⏱️  TIMING: 4019ms
ℹ️  🤖 PROVIDER: gemini / MODEL: gemini-2.5-pro
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
⚠️  ⚙️  TOKEN ADJUSTMENT: 20 → 300 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 💥 TESTING ERROR HANDLING (EXPECTING 400s)
= * 80

- * 60
 Invalid Model Test
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "invalid-model-name",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {}
}

ℹ️  🚀 Sending request...
❌ 💥 Invalid Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Prompt Test
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1"
}

ℹ️  🚀 Sending request...
❌ 💥 Missing Prompt Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Model Test
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk"
}

ℹ️  🚀 Sending request...
❌ 💥 Missing Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

= * 80
 🔧 TESTING SMART TOKEN ALLOCATION
= * 80
ℹ️  Testing provider-specific token minimums...

- * 60
 Gemini - Token Adjustment Verification
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "max_tokens": 30
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=577
📥 RESPONSE: BODY:
{
  "data": {
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]",
    "generationId": "req_8d91dc7df5a1461b"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:43.807Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "userRequested": 30,
      "providerAdjusted": 300,
      "adjustmentReason": "Provider minimum tokens for reliable generation"
    },
    "tokens": {
      "input": 28,
      "output": 0,
      "total": 327,
      "maxRequested": 300
    },
    "provider": "gemini",
    "model": "gemini-2.5-pro",
    "timing": {
      "responseMs": 4057
    }
  },
  "success": true
}

✅ ✨ Gemini - Token Adjustment Verification - PASSED ✨
ℹ️  📝 CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
ℹ️  📏 CONTENT LENGTH: 95 characters
ℹ️  🎯 TOKENS: Input=28, Output=0, Total=327, MaxRequested=300
ℹ️  ⏱️  TIMING: 4057ms
ℹ️  🤖 PROVIDER: gemini / MODEL: gemini-2.5-pro
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
⚠️  ⚙️  TOKEN ADJUSTMENT: 30 → 300 (Provider minimum tokens for reliable generation)
✅ 🎯 SMART TOKEN ALLOCATION WORKING PERFECTLY!
ℹ️  User requested: 30 tokens
ℹ️  Provider used: 300 tokens
ℹ️  Reason: Provider minimum tokens for reliable generation

= * 80
 📐 VALIDATING RESPONSE STRUCTURE
= * 80

- * 60
 Response Structure Validation
- * 60
📤 REQUEST: URL: http://localhost:4000/api/llm/generate
📤 REQUEST: METHOD: POST
📤 REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Please polish this prompt. Make it nicer. Mention time & geography in history. Reply ONLY response. MAX 50 characters: Medieval monk",
  "options": {
    "max_tokens": 50
  }
}

ℹ️  🚀 Sending request...
📥 RESPONSE: STATUS: 200
📥 RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=403
📥 RESPONSE: BODY:
{
  "data": {
    "content": "Medieval monk, specify time & place in history.",
    "generationId": "req_c143c9abcd83dd50"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T04:37:46.67Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 34,
      "output": 11,
      "total": 45,
      "maxRequested": 50
    },
    "provider": "openai",
    "model": "gpt-4.1",
    "timing": {
      "responseMs": 766
    }
  },
  "success": true
}

✅ ✨ Response Structure Validation - PASSED ✨
ℹ️  📝 CONTENT: 'Medieval monk, specify time & place in history.'
ℹ️  📏 CONTENT LENGTH: 47 characters
ℹ️  🎯 TOKENS: Input=34, Output=11, Total=45, MaxRequested=50
ℹ️  ⏱️  TIMING: 766ms
ℹ️  🤖 PROVIDER: openai / MODEL: gpt-4.1
ℹ️  🔧 CONFIG: Temperature=0.7, FinishReason=stop
ℹ️  Checking top-level fields: success, data, metadata
✅ ✅ Found field: success
✅ ✅ Found field: data
✅ ✅ Found field: metadata
ℹ️  Checking data fields: content, generationId
✅ ✅ Found data field: content
✅ ✅ Found data field: generationId
ℹ️  Checking metadata fields: model, provider, tokens, config, timing
✅ ✅ Found metadata field: model
✅ ✅ Found metadata field: provider
✅ ✅ Found metadata field: tokens
✅ ✅ Found metadata field: config
✅ ✅ Found metadata field: timing
✅ 🎉 RESPONSE STRUCTURE VALIDATION PASSED!

= * 80
 🔥 PSYCHO MODE TEST RESULTS SUMMARY 🔥
= * 80

📊 TOTAL TESTS RUN: 15
✅ 🎉 PASSED: 12
❌ 💥 FAILED: 3
📈 SUCCESS RATE: 80%

⚠️  ⚠️  Some tests failed. Review the detailed output above.
ℹ️  🔧 Common issues:
  - Ensure Phoenix server is running (mix phx.server)
  - Check API keys are configured in .env file
  - Verify database is running and migrated

ℹ️  🔥 PSYCHO MODE OPTIONS:
ℹ️  Normal mode (less verbose): .\unifiedAiApi.ps1 -Quiet
ℹ️  Skip models test: .\unifiedAiApi.ps1 -SkipModelsTest
ℹ️  Test production: .\unifiedAiApi.ps1 -BaseUrl 'https://ra-backend.fly.dev'

