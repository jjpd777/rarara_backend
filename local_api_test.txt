
= * 80
 ğŸ”¥ğŸ”¥ğŸ”¥ PSYCHO MODE LLM API TESTING SUITE ğŸ”¥ğŸ”¥ğŸ”¥
= * 80
âš ï¸  PSYCHO MODE ENABLED - SHOWING EVERY REQUEST AND RESPONSE!
â„¹ï¸  Base URL: http://localhost:4000

= * 80
 ğŸ“‹ TESTING MODELS LIST ENDPOINT
= * 80

- * 60
 List Available Models
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/models
ğŸ“¤ REQUEST: METHOD: GET
ğŸ“¤ REQUEST: BODY: (empty)

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=648
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "models": [
      {
        "model": "gpt-4.1",
        "display_name": "OpenAI - GPT-4.1",
        "house": "OpenAI",
        "model_type": "text_gen"
      },
      {
        "model": "claude-sonnet-4-20250514",
        "display_name": "Anthropic - Claude Sonnet 4",
        "house": "Anthropic",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Flash",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5",
        "display_name": "Google - Gemini 2.5",
        "house": "Google",
        "model_type": "text_gen"
      },
      {
        "model": "gemini-2.5-pro",
        "display_name": "Google - Gemini 2.5 Pro",
        "house": "Google",
        "model_type": "text_gen"
      }
    ]
  },
  "metadata": {
    "timestamp": "2025-07-23T03:36:45.731Z",
    "totalCount": 5
  },
  "success": true
}

âœ… âœ¨ List Available Models - PASSED âœ¨
âœ… Found 5 available models:
  ğŸ¤– gpt-4.1 (OpenAI) - OpenAI - GPT-4.1
  ğŸ¤– claude-sonnet-4-20250514 (Anthropic) - Anthropic - Claude Sonnet 4
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Flash
  ğŸ¤– gemini-2.5 (Google) - Google - Gemini 2.5
  ğŸ¤– gemini-2.5-pro (Google) - Google - Gemini 2.5 Pro

= * 80
 ğŸ¤– TESTING OPENAI PROVIDER
= * 80

- * 60
 OpenAI - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=398
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_5734ec65787774c6",
    "content": "Hello, world! ğŸ‘‹ How can I help you today?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:36:48.454Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "stop"
    },
    "tokens": {
      "input": 11,
      "output": 13,
      "total": 24,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 583
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Hello, world! ğŸ‘‹ How can I help you today?'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 42 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=11, Output=13, Total=24, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 583ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=stop

Name                           Value
----                           -----
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Prompt Polishing (YOUR USE CASE)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Improve this character creation prompt. Output only the polished prompt: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=647
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_2598a61610ba5b5e",
    "content": "Create a detailed character profile for a medieval monk. Consider their daily life, religious beliefs, personality traits, appearance, background, and any secret desires or inner conflicts they may have. Include how their monastic duties shape their worldview, and describe the historical"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:36:51.617Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length"
    },
    "tokens": {
      "input": 21,
      "output": 50,
      "total": 71,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1105
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Prompt Polishing (YOUR USE CASE) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Create a detailed character profile for a medieval monk. Consider their daily life, religious belief...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 288 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=21, Output=50, Total=71, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1105ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - Low Token Limit Stress Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Brief response please",
  "options": {
    "temperature": 0.5,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=491
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_744105afca8440ae",
    "content": "Of course! How can I help?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:36:54.137Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.5,
      "finishReason": "stop",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 50,
      "userRequested": 20
    },
    "tokens": {
      "input": 10,
      "output": 8,
      "total": 18,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 437
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - Low Token Limit Stress Test - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Of course! How can I help?'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 26 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=10, Output=8, Total=18, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 437ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.5, FinishReason=stop
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 50 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 OpenAI - No Options (Default Behavior)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Test without options",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=797
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_7bebb6c5fb54a61e",
    "content": "Certainly! If you want to create a test **without options** (such as a multiple-choice test without the answer choices), you can use open-ended questions. Hereâ€™s an example of a short test with questions but **no answer options**:\n\n---\n\n**Sample Test (Without Options):**\n\n1. What is the capital city of France?\n2. Define photosynthesis.\n3"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:36:58.594Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 75
    },
    "tokens": {
      "input": 10,
      "output": 75,
      "total": 85,
      "maxRequested": 75
    },
    "provider": "openai",
    "timing": {
      "responseMs": 2346
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ OpenAI - No Options (Default Behavior) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Certainly! If you want to create a test **without options** (such as a multiple-choice test without ...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 339 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=10, Output=75, Total=85, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 2346ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ§  TESTING ANTHROPIC PROVIDER
= * 80

- * 60
 Anthropic - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=584
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_efccc6a3e081bd1e",
    "content": "Hello! It's nice to meet you. How are you doing today? Is there anything I can help you with?"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:02.527Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "end_turn",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 75,
      "userRequested": 50
    },
    "tokens": {
      "input": 11,
      "output": 27,
      "total": 38,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "timing": {
      "responseMs": 1843
    },
    "model": "claude-sonnet-4-20250514"
  },
  "success": true
}

âœ… âœ¨ Anthropic - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Hello! It's nice to meet you. How are you doing today? Is there anything I can help you with?'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 93 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=11, Output=27, Total=38, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 1843ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=end_turn
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 50 â†’ 75 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Anthropic - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "claude-sonnet-4-20250514",
  "prompt": "Polish this: Medieval monk character",
  "options": {
    "temperature": 0.6,
    "max_tokens": 75
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=711
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_2f82f6102cf968bf",
    "content": "Here are several polished versions of a medieval monk character, depending on your needs:\n\n## **Brother Aldric the Illuminator**\n\n**Background:** A scholarly Benedictine monk in his 40s who serves as the monastery's head scribe and illuminator. Once a minor noble's son, he chose the monastic life after a profound spiritual"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:07.645Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.6,
      "finishReason": "max_tokens"
    },
    "tokens": {
      "input": 13,
      "output": 75,
      "total": 88,
      "maxRequested": 75
    },
    "provider": "anthropic",
    "timing": {
      "responseMs": 3010
    },
    "model": "claude-sonnet-4-20250514"
  },
  "success": true
}

âœ… âœ¨ Anthropic - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Here are several polished versions of a medieval monk character, depending on your needs:

## **Brot...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 324 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=13, Output=75, Total=88, MaxRequested=75
â„¹ï¸  â±ï¸  TIMING: 3010ms
â„¹ï¸  ğŸ¤– PROVIDER: anthropic / MODEL: claude-sonnet-4-20250514
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.6, FinishReason=max_tokens
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’ TESTING GEMINI PROVIDER
= * 80

- * 60
 Gemini - Basic Generation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Hello, world!",
  "options": {
    "temperature": 0.7,
    "max_tokens": 100
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=465
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_c75852bc42a69e36",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:12.62Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 4,
      "output": 0,
      "total": 103,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 2837
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Basic Generation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=4, Output=0, Total=103, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 2837ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Prompt Polishing
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Polish this prompt: Medieval monk",
  "options": {
    "temperature": 0.7,
    "max_tokens": 150
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=465
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_1117ba9a99ea2359",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:17.743Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS"
    },
    "tokens": {
      "input": 6,
      "output": 0,
      "total": 155,
      "maxRequested": 150
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 3017
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Prompt Polishing - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=6, Output=0, Total=155, MaxRequested=150
â„¹ï¸  â±ï¸  TIMING: 3017ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

- * 60
 Gemini - Very Low Tokens (Edge Case)
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Brief response",
  "options": {
    "temperature": 0.7,
    "max_tokens": 20
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=576
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_ea0dbedbb8da5188",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:22.295Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 100,
      "userRequested": 20
    },
    "tokens": {
      "input": 2,
      "output": 0,
      "total": 101,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 2485
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Very Low Tokens (Edge Case) - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=2, Output=0, Total=101, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 2485ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 20 â†’ 100 (Provider minimum tokens for reliable generation)
StatusCode                     200
Success                        True
Response                       @{data=; metadata=; success=True}

= * 80
 ğŸ’¥ TESTING ERROR HANDLING (EXPECTING 400s)
= * 80

- * 60
 Invalid Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "invalid-model-name",
  "prompt": "Test",
  "options": {}
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Invalid Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Prompt Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Prompt Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

- * 60
 Missing Model Test
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "prompt": "test"
}

â„¹ï¸  ğŸš€ Sending request...
âŒ ğŸ’¥ Missing Model Test - EXCEPTION: Response status code does not indicate success: 400 (Bad Request).
Could not parse error response
Error                          Response status code does not indicate success: 400 (Bad Request).
Success                        False

= * 80
 ğŸ”§ TESTING SMART TOKEN ALLOCATION
= * 80
â„¹ï¸  Testing provider-specific token minimums...

- * 60
 Gemini - Token Adjustment Verification
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gemini-2.5-pro",
  "prompt": "Short prompt",
  "options": {
    "max_tokens": 30
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=576
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_12b49cfbe866d06c",
    "content": "[Unable to generate content within token limit - please increase max_tokens or simplify prompt]"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:35.758Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "MAX_TOKENS",
      "adjustmentReason": "Provider minimum tokens for reliable generation",
      "providerAdjusted": 100,
      "userRequested": 30
    },
    "tokens": {
      "input": 2,
      "output": 0,
      "total": 101,
      "maxRequested": 100
    },
    "provider": "gemini",
    "timing": {
      "responseMs": 2010
    },
    "model": "gemini-2.5-pro"
  },
  "success": true
}

âœ… âœ¨ Gemini - Token Adjustment Verification - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: '[Unable to generate content within token limit - please increase max_tokens or simplify prompt]'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 95 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=2, Output=0, Total=101, MaxRequested=100
â„¹ï¸  â±ï¸  TIMING: 2010ms
â„¹ï¸  ğŸ¤– PROVIDER: gemini / MODEL: gemini-2.5-pro
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=MAX_TOKENS
âš ï¸  âš™ï¸  TOKEN ADJUSTMENT: 30 â†’ 100 (Provider minimum tokens for reliable generation)
âœ… ğŸ¯ SMART TOKEN ALLOCATION WORKING PERFECTLY!
â„¹ï¸  User requested: 30 tokens
â„¹ï¸  Provider used: 100 tokens
â„¹ï¸  Reason: Provider minimum tokens for reliable generation

= * 80
 ğŸ“ VALIDATING RESPONSE STRUCTURE
= * 80

- * 60
 Response Structure Validation
- * 60
ğŸ“¤ REQUEST: URL: http://localhost:4000/api/llm/generate
ğŸ“¤ REQUEST: METHOD: POST
ğŸ“¤ REQUEST: BODY:
{
  "model": "gpt-4.1",
  "prompt": "Test structure",
  "options": {
    "max_tokens": 50
  }
}

â„¹ï¸  ğŸš€ Sending request...
ğŸ“¥ RESPONSE: STATUS: 200
ğŸ“¥ RESPONSE: HEADERS: Content-Type=application/json; charset=utf-8, Length=584
ğŸ“¥ RESPONSE: BODY:
{
  "data": {
    "generationId": "req_108493518543f1e7",
    "content": "Certainly! Hereâ€™s a basic outline of a **test structure**. If you have a specific subject or topic in mind, please clarify. For now, hereâ€™s a general template:\n\n---\n\n### Test Structure Example\n\n**1. Test Information**\n-"
  },
  "metadata": {
    "request": {
      "timestamp": "2025-07-23T03:37:39.417Z",
      "attemptNumber": 1,
      "retryCount": 0
    },
    "config": {
      "temperature": 0.7,
      "finishReason": "length"
    },
    "tokens": {
      "input": 9,
      "output": 50,
      "total": 59,
      "maxRequested": 50
    },
    "provider": "openai",
    "timing": {
      "responseMs": 1541
    },
    "model": "gpt-4.1"
  },
  "success": true
}

âœ… âœ¨ Response Structure Validation - PASSED âœ¨
â„¹ï¸  ğŸ“ CONTENT: 'Certainly! Hereâ€™s a basic outline of a **test structure**. If you have a specific subject or topic i...'
â„¹ï¸  ğŸ“ CONTENT LENGTH: 219 characters
â„¹ï¸  ğŸ¯ TOKENS: Input=9, Output=50, Total=59, MaxRequested=50
â„¹ï¸  â±ï¸  TIMING: 1541ms
â„¹ï¸  ğŸ¤– PROVIDER: openai / MODEL: gpt-4.1
â„¹ï¸  ğŸ”§ CONFIG: Temperature=0.7, FinishReason=length
â„¹ï¸  Checking top-level fields: success, data, metadata
âœ… âœ… Found field: success
âœ… âœ… Found field: data
âœ… âœ… Found field: metadata
â„¹ï¸  Checking data fields: content, generationId
âœ… âœ… Found data field: content
âœ… âœ… Found data field: generationId
â„¹ï¸  Checking metadata fields: model, provider, tokens, config, timing
âœ… âœ… Found metadata field: model
âœ… âœ… Found metadata field: provider
âœ… âœ… Found metadata field: tokens
âœ… âœ… Found metadata field: config
âœ… âœ… Found metadata field: timing
âœ… ğŸ‰ RESPONSE STRUCTURE VALIDATION PASSED!

= * 80
 ğŸ”¥ PSYCHO MODE TEST RESULTS SUMMARY ğŸ”¥
= * 80

ğŸ“Š TOTAL TESTS RUN: 15
âœ… ğŸ‰ PASSED: 12
âŒ ğŸ’¥ FAILED: 3
ğŸ“ˆ SUCCESS RATE: 80%

âš ï¸  âš ï¸  Some tests failed. Review the detailed output above.
â„¹ï¸  ğŸ”§ Common issues:
  - Ensure Phoenix server is running (mix phx.server)
  - Check API keys are configured in .env file
  - Verify database is running and migrated

â„¹ï¸  ğŸ”¥ PSYCHO MODE OPTIONS:
â„¹ï¸  Normal mode (less verbose): .\unifiedAiApi.ps1 -Quiet
â„¹ï¸  Skip models test: .\unifiedAiApi.ps1 -SkipModelsTest
â„¹ï¸  Test production: .\unifiedAiApi.ps1 -BaseUrl 'https://ra-backend.fly.dev'

